import sugartensor as tf


num_blocks = 3     # dilated blocks
num_dim = 128/2      # latent dimension
num_channel = 20      # mfcc channel

#
# logit calculating graph using atrous convolution
#
def get_logit(x, voca_size):

    # residual block
    def res_block(tensor, size, rate, block, channel, dim=num_dim):

        with tf.sg_context(name='block%d_%d_%d' % (channel, block, rate)):

            # filter convolution
            conv_filter = tensor.sg_aconv1d(size=size, rate=rate, act='tanh', bn=True, name='conv_filter')

            # gate convolution
            conv_gate = tensor.sg_aconv1d(size=size, rate=rate,  act='sigmoid', bn=True, name='conv_gate')

            # output by gate multiplying
            out = conv_filter * conv_gate

            # final output
            out = out.sg_conv1d(size=1, dim=dim, act='tanh', bn=True, name='conv_out')

            # residual and skip output
            return out + tensor, out

    logit = []
    for ch in range(num_channel):
        # expand dimension
        with tf.sg_context(name='front%d'%ch):
            z = x.sg_conv1d(size=1, dim=num_dim, act='tanh', bn=True, name='conv_in%d'%ch)

        # dilated conv block loop
        skip = 0  # skip connections
        for i in range(num_blocks):
            for r in [1, 2, 4, 8, 16]:
                z, s = res_block(z, size=7, rate=r, block=i, channel=ch)
                skip += s

        # final logit layers
        with tf.sg_context(name='logit%d'%ch):
            logit_ch = (skip
                     .sg_conv1d(size=1, act='tanh', bn=True, name='conv_1%d'%ch)
                     .sg_conv1d(size=1, dim=voca_size, name='conv_2%d'%ch))
        logit.append(logit_ch)

    return tf.stack(logit)
